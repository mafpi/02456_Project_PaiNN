{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYJhCSD4Y9m5"
   },
   "source": [
    "# 02456 Molecular Property Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCIEQk1OY9m-"
   },
   "source": [
    "Basic example of how to train the PaiNN model to predict the QM9 property\n",
    "\"internal energy at 0K\". This property (and the majority of the other QM9\n",
    "properties) is computed as a sum of atomic contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IQzBagr5Y9m_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from tqdm import trange\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import seed_everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkM-l7pcY9nD"
   },
   "source": [
    "## QM9 Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8eDckCCVY9nD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "from typing import Optional, List, Union, Tuple\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "\n",
    "class GetTarget(BaseTransform):\n",
    "    def __init__(self, target: Optional[int] = None) -> None:\n",
    "        self.target = [target]\n",
    "\n",
    "\n",
    "    def forward(self, data: Data) -> Data:\n",
    "        if self.target is not None:\n",
    "            data.y = data.y[:, self.target]\n",
    "        return data\n",
    "\n",
    "\n",
    "class QM9DataModule(pl.LightningDataModule):\n",
    "\n",
    "    target_types = ['atomwise' for _ in range(19)]\n",
    "    target_types[0] = 'dipole_moment'\n",
    "    target_types[5] = 'electronic_spatial_extent'\n",
    "\n",
    "    # Specify unit conversions (eV to meV).\n",
    "    unit_conversion = {\n",
    "        i: (lambda t: 1000*t) if i not in [0, 1, 5, 11, 16, 17, 18]\n",
    "        else (lambda t: t)\n",
    "        for i in range(19)\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target: int = 7,\n",
    "        data_dir: str = 'data/',\n",
    "        batch_size_train: int = 100,\n",
    "        batch_size_inference: int = 1000,\n",
    "        num_workers: int = 0,\n",
    "        splits: Union[List[int], List[float]] = [110000, 10000, 10831],\n",
    "        seed: int = 0,\n",
    "        subset_size: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.target = target\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size_train = batch_size_train\n",
    "        self.batch_size_inference = batch_size_inference\n",
    "        self.num_workers = num_workers\n",
    "        self.splits = splits\n",
    "        self.seed = seed\n",
    "        self.subset_size = subset_size\n",
    "\n",
    "        self.data_train = None\n",
    "        self.data_val = None\n",
    "        self.data_test = None\n",
    "\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Download data\n",
    "        QM9(root=self.data_dir)\n",
    "\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        dataset = QM9(root=self.data_dir, transform=GetTarget(self.target))\n",
    "\n",
    "        # Shuffle dataset\n",
    "        rng = np.random.default_rng(seed=self.seed)\n",
    "        dataset = dataset[torch.tensor(rng.permutation(len(dataset))).long()]\n",
    "\n",
    "        # Subset dataset\n",
    "        if self.subset_size is not None:\n",
    "            dataset = dataset[:self.subset_size]\n",
    "\n",
    "        # Split dataset\n",
    "        if all([type(split) == int for split in self.splits]):\n",
    "            split_sizes = self.splits\n",
    "        elif all([type(split) == float for split in self.splits]):\n",
    "            split_sizes = [int(len(dataset) * prop) for prop in self.splits]\n",
    "\n",
    "        split_idx = np.cumsum(split_sizes)\n",
    "        self.data_train = dataset[:split_idx[0]]\n",
    "        self.data_val = dataset[split_idx[0]:split_idx[1]]\n",
    "        self.data_test = dataset[split_idx[1]:]\n",
    "\n",
    "\n",
    "    def get_target_stats(\n",
    "        self,\n",
    "        remove_atom_refs: bool = True,\n",
    "        divide_by_atoms: bool = True\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "        atom_refs = self.data_train.atomref(self.target)\n",
    "\n",
    "        ys = list()\n",
    "        for batch in self.train_dataloader(shuffle=False):\n",
    "            y = batch.y.clone()\n",
    "            if remove_atom_refs and atom_refs is not None:\n",
    "                y.index_add_(\n",
    "                    dim=0, index=batch.batch, source=-atom_refs[batch.z]\n",
    "                )\n",
    "            if divide_by_atoms:\n",
    "                _, num_atoms  = torch.unique(batch.batch, return_counts=True)\n",
    "                y = y / num_atoms.unsqueeze(-1)\n",
    "            ys.append(y)\n",
    "\n",
    "        y = torch.cat(ys, dim=0)\n",
    "        return y.mean(), y.std(), atom_refs\n",
    "\n",
    "\n",
    "    def train_dataloader(self, shuffle: bool = True) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_train,\n",
    "            batch_size=self.batch_size_train,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_val,\n",
    "            batch_size=self.batch_size_inference,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_test,\n",
    "            batch_size=self.batch_size_inference,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYkLPLVaY9nF"
   },
   "source": [
    "## Post-processing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VvSCFGAVY9nG"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AtomwisePostProcessing(nn.Module):\n",
    "    \"\"\"\n",
    "    Post-processing for (QM9) properties that are predicted as sums of atomic\n",
    "    contributions.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_outputs: int,\n",
    "        mean: torch.FloatTensor,\n",
    "        std: torch.FloatTensor,\n",
    "        atom_refs: torch.FloatTensor,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_outputs: Integer with the number of model outputs. In most\n",
    "                cases 1.\n",
    "            mean: torch.FloatTensor with mean value to shift atomwise\n",
    "                contributions by.\n",
    "            std: torch.FloatTensor with standard deviation to scale atomwise\n",
    "                contributions by.\n",
    "            atom_refs: torch.FloatTensor of size [num_atom_types, 1] with\n",
    "                atomic reference values.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        self.register_buffer('scale', std)\n",
    "        self.register_buffer('shift', mean)\n",
    "        self.atom_refs = nn.Embedding.from_pretrained(atom_refs, freeze=True)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        atomic_contributions: torch.FloatTensor,\n",
    "        atoms: torch.LongTensor,\n",
    "        graph_indexes: torch.LongTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Atomwise post-processing operations and atomic sum.\n",
    "\n",
    "        Args:\n",
    "            atomic_contributions: torch.FloatTensor of size [num_nodes,\n",
    "                num_outputs] with each node's contribution to the overall graph\n",
    "                prediction, i.e., each atom's contribution to the overall\n",
    "                molecular property prediction.\n",
    "            atoms: torch.LongTensor of size [num_nodes] with atom type of each\n",
    "                node in the graph.\n",
    "            graph_indexes: torch.LongTensor of size [num_nodes] with the graph\n",
    "                index each node belongs to.\n",
    "\n",
    "        Returns:\n",
    "            A torch.FLoatTensor of size [num_graphs, num_outputs] with\n",
    "            predictions for each graph (molecule).\n",
    "        \"\"\"\n",
    "        num_graphs = torch.unique(graph_indexes).shape[0]\n",
    "\n",
    "        atomic_contributions = atomic_contributions*self.scale + self.shift\n",
    "        atomic_contributions = atomic_contributions + self.atom_refs(atoms)\n",
    "\n",
    "        # Sum contributions for each graph\n",
    "        output_per_graph = torch.zeros(\n",
    "            (num_graphs, self.num_outputs),\n",
    "            device=atomic_contributions.device,\n",
    "        )\n",
    "        output_per_graph.index_add_(\n",
    "            dim=0,\n",
    "            index=graph_indexes,\n",
    "            source=atomic_contributions,\n",
    "        )\n",
    "\n",
    "        return output_per_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdFpLmi4Y9nH"
   },
   "source": [
    "## PaiNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Pr7ACrkaY9nI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import src.data.AtomNeighbours as AN\n",
    "\n",
    "class PaiNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Polarizable Atom Interaction Neural Network with PyTorch.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_message_passing_layers: int = 3,\n",
    "        num_features: int = 128,\n",
    "        num_outputs: int = 1,\n",
    "        num_rbf_features: int = 20,\n",
    "        num_unique_atoms: int = 100,\n",
    "        cutoff_dist: float = 5.0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_message_passing_layers: Number of message passing layers in\n",
    "                the PaiNN model.\n",
    "            num_features: Size of the node embeddings (scalar features) and\n",
    "                vector features.\n",
    "            num_outputs: Number of model outputs. In most cases 1.\n",
    "            num_rbf_features: Number of radial basis functions to represent\n",
    "                distances.\n",
    "            num_unique_atoms: Number of unique atoms in the data that we want\n",
    "                to learn embeddings for.\n",
    "            cutoff_dist: Euclidean distance threshold for determining whether\n",
    "                two nodes (atoms) are neighbours.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.scalar_embedding = nn.Embedding(num_unique_atoms, num_features)\n",
    "\n",
    "        self.message_layer = nn.ModuleList(\n",
    "            [MessagePaiNN(num_features, num_rbf_features, cutoff_dist) for _ in range(num_message_passing_layers)]\n",
    "        )\n",
    "\n",
    "        self.update_layer = nn.ModuleList(\n",
    "            [UpdatePaiNN(num_features) for _ in range(num_message_passing_layers)]\n",
    "        )\n",
    "\n",
    "        self.last_layer = nn.Sequential(\n",
    "            nn.Linear(num_features, num_features),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(num_features, num_outputs),\n",
    "        )\n",
    "\n",
    "        self.AN = AN.AtomNeighbours(cutoff_dist)\n",
    "\n",
    "        self.num_features = num_features\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        atoms: torch.LongTensor,\n",
    "        atom_positions: torch.FloatTensor,\n",
    "        graph_indexes: torch.LongTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Forward pass of PaiNN. Includes the readout network highlighted in blue\n",
    "        in Figure 2 in (Schütt et al., 2021) with normal linear layers which is\n",
    "        used for predicting properties as sums of atomic contributions. The\n",
    "        post-processing and final sum is perfomed with\n",
    "        src.models.AtomwisePostProcessing.\n",
    "\n",
    "        Args:\n",
    "            atoms: torch.LongTensor of size [num_nodes] with atom type of each\n",
    "                node in the graph.\n",
    "            atom_positions: torch.FloatTensor of size [num_nodes, 3] with\n",
    "                euclidean coordinates of each node / atom.\n",
    "            graph_indexes: torch.LongTensor of size [num_nodes] with the graph\n",
    "                index each node belongs to.\n",
    "\n",
    "        Returns:\n",
    "            A torch.FloatTensor of size [num_nodes, num_outputs] with atomic\n",
    "            contributions to the overall molecular property prediction.\n",
    "        \"\"\"\n",
    "        # Neighbourhood matrix\n",
    "        self.adj_matrix = self.AN.neigbourhood_matrix(atom_positions, graph_indexes)\n",
    "\n",
    "        node_scalar = self.scalar_embedding(atoms)\n",
    "        node_vector = torch.zeros(atoms.size(0), self.num_features, 3, device=atoms.device)\n",
    "\n",
    "        for message_layer, update_layer in zip(self.message_layer, self.update_layer):\n",
    "            node_scalar, node_vector = message_layer(node_scalar, node_vector, self.adj_matrix)\n",
    "            node_scalar, node_vector = update_layer(node_scalar, node_vector)\n",
    "\n",
    "        node_scalar = self.last_layer(node_scalar)\n",
    "\n",
    "\n",
    "        return node_scalar\n",
    "\n",
    "\n",
    "def sinc_expansion(r_ij: torch.Tensor, n: int, cutoff: float):\n",
    "\n",
    "    n_vals = torch.arange(n, device=r_ij.device) + 1\n",
    "\n",
    "    return torch.sin(r_ij.unsqueeze(-1) * n_vals * torch.pi / cutoff) / r_ij.unsqueeze(-1)\n",
    "\n",
    "\n",
    "def cosine_cutoff(r_ij: torch.Tensor, cutoff: float):\n",
    "    return torch.where(\n",
    "        r_ij < cutoff,\n",
    "        0.5 * (torch.cos(torch.pi * r_ij / cutoff) + 1),\n",
    "        torch.tensor(0.0),\n",
    "    )\n",
    "\n",
    "class MessagePaiNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Message passing.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        # num_message_passing_layers: int = 3,\n",
    "        num_features: int = 128,\n",
    "        # num_outputs: int = 1,\n",
    "        num_rbf_features: int = 20,\n",
    "        # num_unique_atoms: int = 100,\n",
    "        cutoff_dist: float = 5.0,\n",
    "    ) -> None:\n",
    "        # \"\"\"\n",
    "        # Args:\n",
    "        #     num_message_passing_layers: Number of message passing layers in\n",
    "        #         the PaiNN model.\n",
    "        #     num_features: Size of the node embeddings (scalar features) and\n",
    "        #         vector features.\n",
    "        #     num_outputs: Number of model outputs. In most cases 1.\n",
    "        #     num_rbf_features: Number of radial basis functions to represent\n",
    "        #         distances.\n",
    "        #     num_unique_atoms: Number of unique atoms in the data that we want\n",
    "        #         to learn embeddings for.\n",
    "        #     cutoff_dist: Euclidean distance threshold for determining whether\n",
    "        #         two nodes (atoms) are neighbours.\n",
    "        # \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.scalar_message = nn.Sequential(\n",
    "            nn.Linear(num_features, num_features),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(num_features, 3 * num_features),\n",
    "        )\n",
    "\n",
    "        self.layer_rbf = nn.Linear(num_rbf_features, 3* num_features)\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.num_rbf_features = num_rbf_features\n",
    "        self.cutoff_dist = cutoff_dist\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_scalar,\n",
    "        node_vector,\n",
    "        adj_matrix\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        xxxx\n",
    "\n",
    "        Args:\n",
    "            djfvkd:ajcnac\n",
    "\n",
    "        Returns:\n",
    "            XXXXX\n",
    "        \"\"\"\n",
    "        atom_scalar = self.scalar_message(node_scalar)\n",
    "        # print(\"atom scalar shape\", atom_scalar.shape)\n",
    "\n",
    "        # RBF\n",
    "\n",
    "        r_ij_dist = adj_matrix[:, 5]\n",
    "\n",
    "        rbf = self.layer_rbf(sinc_expansion(r_ij_dist, self.num_rbf_features, self.cutoff_dist))\n",
    "\n",
    "        rbf_cos_cutoff = rbf * cosine_cutoff(r_ij_dist, self.cutoff_dist).unsqueeze(-1)\n",
    "        # print(\"rbf_cos_cutoff shape\", rbf_cos_cutoff.shape)\n",
    "\n",
    "        # print(\"rbf_cos_cutoff type\", rbf_cos_cutoff.shape)\n",
    "\n",
    "        # print(\"rbf type\", rbf.shape)\n",
    "\n",
    "        # print(\"atom scalar\", atom_scalar[adj_matrix[:, 1].long()].shape)\n",
    "\n",
    "\n",
    "        pre_split = atom_scalar[adj_matrix[:, 1].long()] * rbf_cos_cutoff\n",
    "\n",
    "        # Split\n",
    "        split1, split2, split3 = torch.split(pre_split, self.num_features, dim = -1)\n",
    "\n",
    "        r_ij = adj_matrix[:, 2:5]\n",
    "\n",
    "        r_ij_standardized = r_ij /r_ij_dist.unsqueeze(-1)\n",
    "\n",
    "        # print(\"r_ij_standardized shape\", r_ij_standardized.unsqueeze(1).shape)\n",
    "\n",
    "        message_edge = split3.unsqueeze(-1) * r_ij_standardized.unsqueeze(1)\n",
    "\n",
    "        message_vector = node_vector[adj_matrix[:, 1].long()] * split1.unsqueeze(-1) + message_edge\n",
    "\n",
    "        delta_v = torch.zeros_like(node_vector)\n",
    "        delta_s = torch.zeros_like(node_scalar)\n",
    "\n",
    "        # list_neighbours: index of the neighbours of atom i\n",
    "        delta_s.index_add_(0, adj_matrix[:, 0].long(), split2)\n",
    "        delta_v.index_add_(0, adj_matrix[:, 0].long(), message_vector)\n",
    "\n",
    "        return node_scalar + delta_s, node_vector + delta_v\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class UpdatePaiNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Update passing.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        # num_message_passing_layers: int = 3,\n",
    "        num_features: int = 128,\n",
    "        # num_outputs: int = 1,\n",
    "        # num_rbf_features: int = 20,\n",
    "        # num_unique_atoms: int = 100,\n",
    "        # cutoff_dist: float = 5.0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.update_U = nn.Linear(num_features, num_features, bias=False)\n",
    "        self.update_V = nn.Linear(num_features, num_features, bias=False)\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.scalar_update = nn.Sequential(\n",
    "            nn.Linear(num_features * 2, num_features),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(num_features, 3 * num_features),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_scalar,\n",
    "        node_vector\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        xxxx\n",
    "\n",
    "        Args:\n",
    "            djfvkd:ajcnac\n",
    "\n",
    "        Returns:\n",
    "            XXXXX\n",
    "        \"\"\"\n",
    "        U = self.update_U(node_vector.permute(0, 2, 1))\n",
    "        V = self.update_V(node_vector.permute(0, 2, 1))\n",
    "        U = U.permute(0,2,1)\n",
    "        V = V.permute(0,2,1)\n",
    "\n",
    "        V_norm = torch.norm(V, dim = -1)\n",
    "\n",
    "        pre_split_s = self.scalar_update(torch.cat((V_norm, node_scalar), dim = 1))\n",
    "\n",
    "        a_vv, a_sv, a_ss = torch.split(pre_split_s, self.num_features, dim = 1)\n",
    "\n",
    "        delta_v = a_vv.unsqueeze(2) * U\n",
    "\n",
    "        inner_prod = torch.sum(U * V, dim=2)\n",
    "\n",
    "        delta_s = inner_prod * a_sv + a_ss\n",
    "\n",
    "        return node_scalar + delta_s, node_vector + delta_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N71kUqtKY9nK"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pMS23pibY9nL"
   },
   "outputs": [],
   "source": [
    "def cli(args: list = []):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--seed', default=0)\n",
    "\n",
    "    # Data\n",
    "    parser.add_argument('--target', default=7, type=int) # 7 => Internal energy at 0K\n",
    "    parser.add_argument('--data_dir', default='data/', type=str)\n",
    "    parser.add_argument('--batch_size_train', default=100, type=int)\n",
    "    parser.add_argument('--batch_size_inference', default=1000, type=int)\n",
    "    parser.add_argument('--num_workers', default=0, type=int)\n",
    "    parser.add_argument('--splits', nargs=3, default=[110000, 10000, 10831], type=int) # [num_train, num_val, num_test]\n",
    "    parser.add_argument('--subset_size', default=None, type=int)\n",
    "\n",
    "    # Model\n",
    "    parser.add_argument('--num_message_passing_layers', default=3, type=int)\n",
    "    parser.add_argument('--num_features', default=128, type=int)\n",
    "    parser.add_argument('--num_outputs', default=1, type=int)\n",
    "    parser.add_argument('--num_rbf_features', default=20, type=int)\n",
    "    parser.add_argument('--num_unique_atoms', default=100, type=int)\n",
    "    parser.add_argument('--cutoff_dist', default=5.0, type=float)\n",
    "\n",
    "    # Training\n",
    "    parser.add_argument('--lr', default=5e-4, type=float)\n",
    "    parser.add_argument('--weight_decay', default=0.01, type=float)\n",
    "    parser.add_argument('--num_epochs', default=3, type=int)\n",
    "    parser.add_argument('--patience', default=30, type=int)\n",
    "    parser.add_argument('--swa_lr', default=0.0001, type=float)\n",
    "\n",
    "    args = parser.parse_args(args=args)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4Q1t6pWY9nM"
   },
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GU6HA6BlY9nN",
    "outputId": "52dc8572-0cee-4f4b-c0ae-658e479f3e14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    }
   ],
   "source": [
    "args = [] # Specify non-default arguments in this list\n",
    "args = cli(args)\n",
    "seed_everything(args.seed)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "dm = QM9DataModule(\n",
    "    target=args.target,\n",
    "    data_dir=args.data_dir,\n",
    "    batch_size_train=args.batch_size_train,\n",
    "    batch_size_inference=args.batch_size_inference,\n",
    "    num_workers=args.num_workers,\n",
    "    splits=args.splits,\n",
    "    seed=args.seed,\n",
    "    subset_size=args.subset_size,\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "y_mean, y_std, atom_refs = dm.get_target_stats(\n",
    "    remove_atom_refs=True, divide_by_atoms=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "l38bxxRYY9nP",
    "outputId": "53171462-f434-4554-f191-dda02df8e3df"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:32<00:00,  1.84s/it, Train loss: 3.484e-08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE (SWA): 7.835\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR, update_bn\n",
    "\n",
    "painn = PaiNN(\n",
    "    num_message_passing_layers=args.num_message_passing_layers,\n",
    "    num_features=args.num_features,\n",
    "    num_outputs=args.num_outputs,\n",
    "    num_rbf_features=args.num_rbf_features,\n",
    "    num_unique_atoms=args.num_unique_atoms,\n",
    "    cutoff_dist=args.cutoff_dist,\n",
    ").to(device)\n",
    "\n",
    "# Load the pre-trained model weights\n",
    "painn.load_state_dict(torch.load(\"best_model_3_layer.pth\", map_location = device))\n",
    "\n",
    "post_processing = AtomwisePostProcessing(\n",
    "    args.num_outputs, y_mean, y_std, atom_refs\n",
    ").to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(painn.parameters(), lr=args.swa_lr, momentum=0.9)\n",
    "\n",
    "# Wrap PaiNN with SWA\n",
    "swa_model = AveragedModel(painn).to(device)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "# Training Loop\n",
    "painn.train()\n",
    "pbar = trange(50)\n",
    "for epoch in pbar:\n",
    "    loss_epoch = 0.\n",
    "    for batch in dm.train_dataloader():\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        atomic_contributions = painn(\n",
    "            atoms=batch.z,\n",
    "            atom_positions=batch.pos,\n",
    "            graph_indexes=batch.batch\n",
    "        )\n",
    "        preds = post_processing(\n",
    "            atoms=batch.z,\n",
    "            graph_indexes=batch.batch,\n",
    "            atomic_contributions=atomic_contributions,\n",
    "        )\n",
    "\n",
    "        loss_step = F.mse_loss(preds, batch.y, reduction='sum')\n",
    "        loss = loss_step / len(batch.y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch += loss_step.detach().item()\n",
    "        break \n",
    "    loss_epoch /= len(dm.data_train)\n",
    "    train_losses.append(loss_epoch)\n",
    "\n",
    "    pbar.set_postfix_str(f'Train loss: {loss_epoch:.3e}')\n",
    "\n",
    "    swa_model.update_parameters(painn)\n",
    "\n",
    "# Evaluate SWA model\n",
    "swa_model.eval()\n",
    "mae = 0\n",
    "with torch.no_grad():\n",
    "    for batch in dm.test_dataloader():\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        atomic_contributions = swa_model(\n",
    "            atoms=batch.z,\n",
    "            atom_positions=batch.pos,\n",
    "            graph_indexes=batch.batch,\n",
    "        )\n",
    "        preds = post_processing(\n",
    "            atoms=batch.z,\n",
    "            graph_indexes=batch.batch,\n",
    "            atomic_contributions=atomic_contributions,\n",
    "        )\n",
    "        mae += F.l1_loss(preds, batch.y, reduction='sum')\n",
    "\n",
    "mae /= len(dm.data_test)\n",
    "unit_conversion = dm.unit_conversion[args.target]\n",
    "print(f'Test MAE (SWA): {unit_conversion(mae):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "GLkJz8_Kgwg7",
    "outputId": "e362b43f-c755-432f-8d12-54dcf92bd6fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:14<00:00,  4.97s/it, Train loss: 1.037e-07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE (SWAG): 0.790\n"
     ]
    }
   ],
   "source": [
    "from swag.posteriors.swag import SWAG\n",
    "import torch.optim as optim\n",
    "\n",
    "painn = PaiNN(\n",
    "    num_message_passing_layers=args.num_message_passing_layers,\n",
    "    num_features=args.num_features,\n",
    "    num_outputs=args.num_outputs,\n",
    "    num_rbf_features=args.num_rbf_features,\n",
    "    num_unique_atoms=args.num_unique_atoms,\n",
    "    cutoff_dist=args.cutoff_dist,\n",
    ").to(device)\n",
    "\n",
    "# dict of args\n",
    "dict_args = {\n",
    "    'atoms': torch.zeros((1,), dtype=torch.long).to(device),\n",
    "    'atom_positions': torch.zeros((1, 3), dtype=torch.float32).to(device),\n",
    "    'graph_indexes': torch.zeros((1,), dtype=torch.long).to(device),\n",
    "}\n",
    "\n",
    "# Load the pre-trained model weights\n",
    "painn.load_state_dict(torch.load(\"best_model_3_layer.pth\", map_location = device))\n",
    "\n",
    "post_processing = AtomwisePostProcessing(\n",
    "    args.num_outputs, y_mean, y_std, atom_refs\n",
    ").to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(painn.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "# Wrap PaiNN with SWA\n",
    "swag_model = SWAG(\n",
    "    PaiNN).to(device)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "# Training Loop\n",
    "painn.train()\n",
    "pbar = trange(3)\n",
    "for epoch in pbar:\n",
    "    loss_epoch = 0.\n",
    "    batch_counter = 0\n",
    "    for batch in dm.train_dataloader():\n",
    "        batch = batch.to(device)\n",
    "        if batch_counter >= 3:  # Break after processing 3 batches\n",
    "            break\n",
    "\n",
    "        atomic_contributions = painn(\n",
    "            atoms=batch.z,\n",
    "            atom_positions=batch.pos,\n",
    "            graph_indexes=batch.batch\n",
    "        )\n",
    "        preds = post_processing(\n",
    "            atoms=batch.z,\n",
    "            graph_indexes=batch.batch,\n",
    "            atomic_contributions=atomic_contributions,\n",
    "        )\n",
    "\n",
    "        loss_step = F.mse_loss(preds, batch.y, reduction='sum')\n",
    "        loss = loss_step / len(batch.y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch += loss_step.detach().item()\n",
    "        batch_counter += 1\n",
    "    loss_epoch /= len(dm.data_train)\n",
    "    train_losses.append(loss_epoch)\n",
    "    \n",
    "    pbar.set_postfix_str(f'Train loss: {loss_epoch:.3e}')\n",
    "\n",
    "    swag_model.collect_model(painn)\n",
    "\n",
    "\n",
    "# Evaluate SWAG model\n",
    "# Sample weights from SWAG posterior\n",
    "num_samples = 10  # Number of posterior samples\n",
    "swag_model.eval()\n",
    "mae = 0\n",
    "for i in range(num_samples):\n",
    "    swag_model.sample()  # Sample weights\n",
    "\n",
    "    # Evaluate sampled model\n",
    "    with torch.no_grad():\n",
    "        for batch in dm.test_dataloader():\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            atomic_contributions = swag_model(\n",
    "                atoms=batch.z,\n",
    "                atom_positions=batch.pos,\n",
    "                graph_indexes=batch.batch,\n",
    "            )\n",
    "            preds = post_processing(\n",
    "                atoms=batch.z,\n",
    "                graph_indexes=batch.batch,\n",
    "                atomic_contributions=atomic_contributions,\n",
    "            )\n",
    "            mae += F.l1_loss(preds, batch.y, reduction='sum')\n",
    "        break\n",
    "\n",
    "\n",
    "mae /= (len(dm.data_test) * num_samples)\n",
    "unit_conversion = dm.unit_conversion[args.target]\n",
    "print(f'Test MAE (SWAG): {unit_conversion(mae):.3f}')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
