{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02456 Molecular Property Prediction - SWAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the PaiNN model to predict the QM9 property\n",
    "\"internal energy at 0K\". This property (and the majority of the other QM9\n",
    "properties) is computed as a sum of atomic contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from tqdm import trange\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import seed_everything\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from swag.posteriors.swag import SWAG\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QM9 Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "from typing import Optional, List, Union, Tuple\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "\n",
    "class GetTarget(BaseTransform):\n",
    "    def __init__(self, target: Optional[int] = None) -> None:\n",
    "        self.target = [target]\n",
    "\n",
    "\n",
    "    def forward(self, data: Data) -> Data:\n",
    "        if self.target is not None:\n",
    "            data.y = data.y[:, self.target]\n",
    "        return data\n",
    "\n",
    "\n",
    "class QM9DataModule(pl.LightningDataModule):\n",
    "\n",
    "    target_types = ['atomwise' for _ in range(19)]\n",
    "    target_types[0] = 'dipole_moment'\n",
    "    target_types[5] = 'electronic_spatial_extent'\n",
    "\n",
    "    # Specify unit conversions (eV to meV).\n",
    "    unit_conversion = {\n",
    "        i: (lambda t: 1000*t) if i not in [0, 1, 5, 11, 16, 17, 18]\n",
    "        else (lambda t: t)\n",
    "        for i in range(19)\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target: int = 7,\n",
    "        data_dir: str = 'data/',\n",
    "        batch_size_train: int = 100,\n",
    "        batch_size_inference: int = 1000,\n",
    "        num_workers: int = 0,\n",
    "        splits: Union[List[int], List[float]] = [110000, 10000, 10831],\n",
    "        seed: int = 0,\n",
    "        subset_size: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.target = target\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size_train = batch_size_train\n",
    "        self.batch_size_inference = batch_size_inference\n",
    "        self.num_workers = num_workers\n",
    "        self.splits = splits\n",
    "        self.seed = seed\n",
    "        self.subset_size = subset_size\n",
    "\n",
    "        self.data_train = None\n",
    "        self.data_val = None\n",
    "        self.data_test = None\n",
    "\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Download data\n",
    "        QM9(root=self.data_dir)\n",
    "\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        dataset = QM9(root=self.data_dir, transform=GetTarget(self.target))\n",
    "\n",
    "        # Shuffle dataset\n",
    "        rng = np.random.default_rng(seed=self.seed)\n",
    "        dataset = dataset[torch.tensor(rng.permutation(len(dataset))).long()]\n",
    "\n",
    "        # Subset dataset\n",
    "        if self.subset_size is not None:\n",
    "            dataset = dataset[:self.subset_size]\n",
    "        \n",
    "        # Split dataset\n",
    "        if all([type(split) == int for split in self.splits]):\n",
    "            split_sizes = self.splits\n",
    "        elif all([type(split) == float for split in self.splits]):\n",
    "            split_sizes = [int(len(dataset) * prop) for prop in self.splits]\n",
    "\n",
    "        split_idx = np.cumsum(split_sizes)\n",
    "        self.data_train = dataset[:split_idx[0]]\n",
    "        self.data_val = dataset[split_idx[0]:split_idx[1]]\n",
    "        self.data_test = dataset[split_idx[1]:]\n",
    "\n",
    "\n",
    "    def get_target_stats(\n",
    "        self,\n",
    "        remove_atom_refs: bool = True,\n",
    "        divide_by_atoms: bool = True\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "        atom_refs = self.data_train.atomref(self.target)\n",
    "\n",
    "        ys = list()\n",
    "        for batch in self.train_dataloader(shuffle=False):\n",
    "            y = batch.y.clone()\n",
    "            if remove_atom_refs and atom_refs is not None:\n",
    "                y.index_add_(\n",
    "                    dim=0, index=batch.batch, source=-atom_refs[batch.z]\n",
    "                )\n",
    "            if divide_by_atoms:\n",
    "                _, num_atoms  = torch.unique(batch.batch, return_counts=True)\n",
    "                y = y / num_atoms.unsqueeze(-1)\n",
    "            ys.append(y)\n",
    "\n",
    "        y = torch.cat(ys, dim=0)\n",
    "        return y.mean(), y.std(), atom_refs\n",
    "\n",
    "\n",
    "    def train_dataloader(self, shuffle: bool = True) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_train,\n",
    "            batch_size=self.batch_size_train,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_val,\n",
    "            batch_size=self.batch_size_inference,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_test,\n",
    "            batch_size=self.batch_size_inference,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AtomwisePostProcessing(nn.Module):\n",
    "    \"\"\"\n",
    "    Post-processing for (QM9) properties that are predicted as sums of atomic\n",
    "    contributions.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_outputs: int,\n",
    "        mean: torch.FloatTensor,\n",
    "        std: torch.FloatTensor,\n",
    "        atom_refs: torch.FloatTensor,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_outputs: Integer with the number of model outputs. In most\n",
    "                cases 1.\n",
    "            mean: torch.FloatTensor with mean value to shift atomwise\n",
    "                contributions by.\n",
    "            std: torch.FloatTensor with standard deviation to scale atomwise\n",
    "                contributions by.\n",
    "            atom_refs: torch.FloatTensor of size [num_atom_types, 1] with\n",
    "                atomic reference values.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        self.register_buffer('scale', std)\n",
    "        self.register_buffer('shift', mean)\n",
    "        self.atom_refs = nn.Embedding.from_pretrained(atom_refs, freeze=True)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        atomic_contributions: torch.FloatTensor,\n",
    "        atoms: torch.LongTensor,\n",
    "        graph_indexes: torch.LongTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Atomwise post-processing operations and atomic sum.\n",
    "\n",
    "        Args:\n",
    "            atomic_contributions: torch.FloatTensor of size [num_nodes,\n",
    "                num_outputs] with each node's contribution to the overall graph\n",
    "                prediction, i.e., each atom's contribution to the overall\n",
    "                molecular property prediction.\n",
    "            atoms: torch.LongTensor of size [num_nodes] with atom type of each\n",
    "                node in the graph.\n",
    "            graph_indexes: torch.LongTensor of size [num_nodes] with the graph \n",
    "                index each node belongs to.\n",
    "\n",
    "        Returns:\n",
    "            A torch.FLoatTensor of size [num_graphs, num_outputs] with\n",
    "            predictions for each graph (molecule).\n",
    "        \"\"\"\n",
    "        num_graphs = torch.unique(graph_indexes).shape[0]\n",
    "\n",
    "        atomic_contributions = atomic_contributions*self.scale + self.shift\n",
    "        atomic_contributions = atomic_contributions + self.atom_refs(atoms)\n",
    "\n",
    "        # Sum contributions for each graph\n",
    "        output_per_graph = torch.zeros(\n",
    "            (num_graphs, self.num_outputs),\n",
    "            device=atomic_contributions.device,\n",
    "        )\n",
    "        output_per_graph.index_add_(\n",
    "            dim=0,\n",
    "            index=graph_indexes,\n",
    "            source=atomic_contributions,\n",
    "        )\n",
    "\n",
    "        return output_per_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PaiNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import src.data.AtomNeighbours as AN\n",
    "\n",
    "class PaiNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Polarizable Atom Interaction Neural Network with PyTorch.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_message_passing_layers: int = 3,\n",
    "        num_features: int = 128,\n",
    "        num_outputs: int = 1,\n",
    "        num_rbf_features: int = 20,\n",
    "        num_unique_atoms: int = 100,\n",
    "        cutoff_dist: float = 5.0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_message_passing_layers: Number of message passing layers in\n",
    "                the PaiNN model.\n",
    "            num_features: Size of the node embeddings (scalar features) and\n",
    "                vector features.\n",
    "            num_outputs: Number of model outputs. In most cases 1.\n",
    "            num_rbf_features: Number of radial basis functions to represent\n",
    "                distances.\n",
    "            num_unique_atoms: Number of unique atoms in the data that we want\n",
    "                to learn embeddings for.\n",
    "            cutoff_dist: Euclidean distance threshold for determining whether \n",
    "                two nodes (atoms) are neighbours.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # scalar embedding\n",
    "        self.scalar_embedding = nn.Embedding(num_unique_atoms, num_features)\n",
    "\n",
    "        # message passing layers\n",
    "        self.message_layer = nn.ModuleList(\n",
    "            [MessagePaiNN(num_features, num_rbf_features, cutoff_dist) for _ in range(num_message_passing_layers)]\n",
    "        )\n",
    "\n",
    "        # update layers\n",
    "        self.update_layer = nn.ModuleList(\n",
    "            [UpdatePaiNN(num_features) for _ in range(num_message_passing_layers)]\n",
    "        )\n",
    "\n",
    "        # readout network\n",
    "        self.last_layer = nn.Sequential(\n",
    "            nn.Linear(num_features, num_features),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(num_features, num_outputs),\n",
    "        )\n",
    "        \n",
    "        # Atom Neighbours - adjacency matrix\n",
    "        self.AN = AN.AtomNeighbours(cutoff_dist)\n",
    "\n",
    "        self.num_features = num_features\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        atoms: torch.LongTensor,\n",
    "        atom_positions: torch.FloatTensor,\n",
    "        graph_indexes: torch.LongTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Forward pass of PaiNN. Includes the readout network highlighted in blue\n",
    "        in Figure 2 in (Schütt et al., 2021) with normal linear layers which is\n",
    "        used for predicting properties as sums of atomic contributions. The\n",
    "        post-processing and final sum is perfomed with\n",
    "        src.models.AtomwisePostProcessing.\n",
    "\n",
    "        Args:\n",
    "            atoms: torch.LongTensor of size [num_nodes] with atom type of each\n",
    "                node in the graph.\n",
    "            atom_positions: torch.FloatTensor of size [num_nodes, 3] with\n",
    "                euclidean coordinates of each node / atom.\n",
    "            graph_indexes: torch.LongTensor of size [num_nodes] with the graph \n",
    "                index each node belongs to.\n",
    "\n",
    "        Returns:\n",
    "            A torch.FloatTensor of size [num_nodes, num_outputs] with atomic\n",
    "            contributions to the overall molecular property prediction.\n",
    "        \"\"\"\n",
    "\n",
    "        # Neighbourhood matrix for a set of atoms\n",
    "        self.adj_matrix = self.AN.neigbourhood_matrix(atom_positions, graph_indexes)\n",
    "\n",
    "        node_scalar = self.scalar_embedding(atoms)\n",
    "        node_vector = torch.zeros(atoms.size(0), self.num_features, 3)\n",
    "\n",
    "        for message_layer, update_layer in zip(self.message_layer, self.update_layer):\n",
    "            node_scalar, node_vector = message_layer(node_scalar, node_vector, self.adj_matrix)\n",
    "            node_scalar, node_vector = update_layer(node_scalar, node_vector)\n",
    "\n",
    "        node_scalar = self.last_layer(node_scalar)\n",
    "\n",
    "\n",
    "        return node_scalar\n",
    "\n",
    "\n",
    "def sinc_expansion(r_ij: torch.Tensor, n: int, cutoff: float):\n",
    "    \"\"\"\n",
    "    Sinc expansion of distances.\n",
    "\n",
    "    Args:\n",
    "        r_ij: Pairwise relative position between atoms.\n",
    "        n: Number of radial basis functions.\n",
    "        cutoff: Cutoff distance for the radial basis functions.\n",
    "\n",
    "    Returns:\n",
    "        A torch.Tensor of size [num_atoms, n] with the sinc expansion of the\n",
    "        distances.\n",
    "    \n",
    "    \"\"\"\n",
    "    n_vals = torch.arange(n) + 1\n",
    "\n",
    "    return torch.sin(r_ij.unsqueeze(-1) * n_vals * torch.pi / cutoff) / r_ij.unsqueeze(-1)\n",
    "\n",
    "\n",
    "def cosine_cutoff(r_ij: torch.Tensor, cutoff: float):\n",
    "    \"\"\"\"\n",
    "    Cosine cutoff function.\n",
    "\n",
    "    Args:\n",
    "        r_ij: Pairwise relative position between atoms.\n",
    "        cutoff: Cutoff distance for the radial basis functions.\n",
    "\n",
    "    Returns:\n",
    "        A torch.Tensor of size [num_atoms] with the cosine cutoff function.\n",
    "    \n",
    "    \"\"\"\n",
    "    return torch.where(\n",
    "        r_ij < cutoff,\n",
    "        0.5 * (torch.cos(torch.pi * r_ij / cutoff) + 1),\n",
    "        torch.tensor(0.0),\n",
    "    )\n",
    "\n",
    "class MessagePaiNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Message passing layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int = 128,\n",
    "        num_rbf_features: int = 20,\n",
    "        cutoff_dist: float = 5.0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "            num_features: Size of the node embeddings (scalar features) and\n",
    "                vector features.\n",
    "            num_rbf_features: Number of radial basis functions to represent\n",
    "                distances.\n",
    "            cutoff_dist: Euclidean distance threshold for determining whether \n",
    "                two nodes (atoms) are neighbours.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.scalar_message = nn.Sequential(\n",
    "            nn.Linear(num_features, num_features),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(num_features, 3 * num_features),\n",
    "        )\n",
    "\n",
    "        self.layer_rbf = nn.Linear(num_rbf_features, 3* num_features)\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.num_rbf_features = num_rbf_features\n",
    "        self.cutoff_dist = cutoff_dist\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_scalar,\n",
    "        node_vector,\n",
    "        adj_matrix\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Forward pass of Message layer in PaiNN. Includes the layers highlighted in orange\n",
    "        in Figure 2 in (Schütt et al., 2021).\n",
    "        \n",
    "        Args:\n",
    "            node_scalar: torch.FloatTensor of size [num_atoms, num_features]\n",
    "                with scalar features for each atom.\n",
    "            node_vector: torch.FloatTensor of size [num_atoms, num_features, 3]\n",
    "                with vector features for each atom.\n",
    "            adj_matrix: torch.LongTensor of size [num_edges, 6] with the\n",
    "                adjacency matrix. The columns are [node_i, node_j, x_ij, y_ij,\n",
    "                z_ij, r_ij].\n",
    "\n",
    "        Returns:\n",
    "            A tuple with two torch.FloatTensor of size [num_atoms, num_features]\n",
    "            and [num_atoms, num_features, 3] with updated scalar and vector\n",
    "            features, respectively\n",
    "        \"\"\"\n",
    "        atom_scalar = self.scalar_message(node_scalar)\n",
    "\n",
    "        # RBF\n",
    "        r_ij_dist = adj_matrix[:, 5]\n",
    "        rbf = self.layer_rbf(sinc_expansion(r_ij_dist, self.num_rbf_features, self.cutoff_dist)) \n",
    "        rbf_cos_cutoff = rbf * cosine_cutoff(r_ij_dist, self.cutoff_dist).unsqueeze(-1)\n",
    "\n",
    "        pre_split = atom_scalar[adj_matrix[:, 1].long()] * rbf_cos_cutoff\n",
    "        # Split\n",
    "        split1, split2, split3 = torch.split(pre_split, self.num_features, dim = -1)\n",
    "\n",
    "        r_ij = adj_matrix[:, 2:5]\n",
    "        r_ij_standardized = r_ij /r_ij_dist.unsqueeze(-1) \n",
    "        \n",
    "        message_edge = split3.unsqueeze(-1) * r_ij_standardized.unsqueeze(1)\n",
    "\n",
    "        message_vector = node_vector[adj_matrix[:, 1].long()] * split1.unsqueeze(-1) + message_edge\n",
    "\n",
    "        delta_v = torch.zeros_like(node_vector)\n",
    "        delta_s = torch.zeros_like(node_scalar)\n",
    "\n",
    "        # list_neighbours: index of the neighbours of atom i\n",
    "        delta_s.index_add_(0, adj_matrix[:, 0].long(), split2)\n",
    "        delta_v.index_add_(0, adj_matrix[:, 0].long(), message_vector)        \n",
    "\n",
    "        return node_scalar + delta_s, node_vector + delta_v\n",
    "    \n",
    "\n",
    "class UpdatePaiNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Update layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int = 128,\n",
    "    ) -> None:\n",
    "        \"\"\"\"\n",
    "        Args:\n",
    "            num_features: Size of the node embeddings (scalar features) and\n",
    "                vector features.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.update_U = nn.Linear(num_features, num_features, bias=False)\n",
    "        self.update_V = nn.Linear(num_features, num_features, bias=False)\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.scalar_update = nn.Sequential(\n",
    "            nn.Linear(num_features * 2, num_features),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(num_features, 3 * num_features),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_scalar,\n",
    "        node_vector\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Forward pass of Update layer in PaiNN. Includes the layers highlighted in yellow\n",
    "        in Figure 2 in (Schütt et al., 2021).\n",
    "\n",
    "        Args:\n",
    "            node_scalar: torch.FloatTensor of size [num_atoms, num_features]\n",
    "                with scalar features for each atom.\n",
    "            node_vector: torch.FloatTensor of size [num_atoms, num_features, 3]\n",
    "                with vector features for each atom.\n",
    "\n",
    "        Returns:\n",
    "            A tuple with two torch.FloatTensor of size [num_atoms, num_features]\n",
    "            and [num_atoms, num_features, 3] with updated scalar and vector\n",
    "            features, respectively\n",
    "        \"\"\"\n",
    "\n",
    "        U = self.update_U(node_vector.permute(0, 2, 1))\n",
    "        V = self.update_V(node_vector.permute(0, 2, 1))\n",
    "        U = U.permute(0,2,1)\n",
    "        V = V.permute(0,2,1)\n",
    "\n",
    "        V_norm = torch.norm(V, dim = -1)\n",
    "      \n",
    "        pre_split_s = self.scalar_update(torch.cat((V_norm, node_scalar), dim = 1))\n",
    "\n",
    "        a_vv, a_sv, a_ss = torch.split(pre_split_s, self.num_features, dim = 1)\n",
    "\n",
    "        delta_v = a_vv.unsqueeze(2) * U\n",
    "\n",
    "        inner_prod = torch.sum(U * V, dim=2)\n",
    "        \n",
    "        delta_s = inner_prod * a_sv + a_ss  \n",
    "\n",
    "        return node_scalar + delta_s, node_vector + delta_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cli(args: list = []):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--seed', default=0)\n",
    "\n",
    "    # Data\n",
    "    parser.add_argument('--target', default=7, type=int) # 7 => Internal energy at 0K\n",
    "    parser.add_argument('--data_dir', default='data/', type=str)\n",
    "    parser.add_argument('--batch_size_train', default=100, type=int)\n",
    "    parser.add_argument('--batch_size_inference', default=1000, type=int)\n",
    "    parser.add_argument('--num_workers', default=0, type=int)\n",
    "    parser.add_argument('--splits', nargs=3, default=[110000, 10000, 10831], type=int) # [num_train, num_val, num_test]\n",
    "    parser.add_argument('--subset_size', default=None, type=int)\n",
    "\n",
    "    # Model\n",
    "    parser.add_argument('--num_message_passing_layers', default=3, type=int)\n",
    "    parser.add_argument('--num_features', default=128, type=int)\n",
    "    parser.add_argument('--num_outputs', default=1, type=int)\n",
    "    parser.add_argument('--num_rbf_features', default=20, type=int)\n",
    "    parser.add_argument('--num_unique_atoms', default=100, type=int)\n",
    "    parser.add_argument('--cutoff_dist', default=5.0, type=float)\n",
    "\n",
    "    # Training\n",
    "    parser.add_argument('--lr', default=5e-4, type=float)\n",
    "    parser.add_argument('--weight_decay', default=0.01, type=float)\n",
    "    parser.add_argument('--num_epochs', default=1000, type=int)\n",
    "    parser.add_argument('--patience', default=30, type=int)\n",
    "    parser.add_argument('--swag_lr', default=0.0001, type=float)\n",
    "\n",
    "    args = parser.parse_args(args=args)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    }
   ],
   "source": [
    "args = [] # Specify non-default arguments in this list\n",
    "args = cli(args)\n",
    "seed_everything(args.seed)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "dm = QM9DataModule(\n",
    "    target=args.target,\n",
    "    data_dir=args.data_dir,\n",
    "    batch_size_train=args.batch_size_train,\n",
    "    batch_size_inference=args.batch_size_inference,\n",
    "    num_workers=args.num_workers,\n",
    "    splits=args.splits,\n",
    "    seed=args.seed,\n",
    "    subset_size=args.subset_size,\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "y_mean, y_std, atom_refs = dm.get_target_stats(\n",
    "    remove_atom_refs=True, divide_by_atoms=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "painn = PaiNN(\n",
    "    num_message_passing_layers=args.num_message_passing_layers,\n",
    "    num_features=args.num_features,\n",
    "    num_outputs=args.num_outputs, \n",
    "    num_rbf_features=args.num_rbf_features,\n",
    "    num_unique_atoms=args.num_unique_atoms,\n",
    "    cutoff_dist=args.cutoff_dist,\n",
    ")\n",
    "post_processing = AtomwisePostProcessing(\n",
    "    args.num_outputs, y_mean, y_std, atom_refs\n",
    ")\n",
    "\n",
    "# Load the pre-trained model weights\n",
    "painn.load_state_dict(torch.load(\"best_model.pth\", map_location = device))\n",
    "\n",
    "painn.to(device)\n",
    "post_processing.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(painn.parameters(), lr=args.swag_lr, momentum=0.9)\n",
    "\n",
    "# Wrap PaiNN with SWAG\n",
    "swag_model = SWAG(PaiNN,\n",
    "    num_message_passing_layers=args.num_message_passing_layers,\n",
    "    num_features=args.num_features,\n",
    "    num_outputs=args.num_outputs,\n",
    "    num_rbf_features=args.num_rbf_features,\n",
    "    num_unique_atoms=args.num_unique_atoms,\n",
    "    cutoff_dist=args.cutoff_dist,\n",
    "    no_cov_mat = False\n",
    "    ).to(device)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "# Training Loop\n",
    "painn.train()\n",
    "pbar = trange(args.num_epochs)\n",
    "for epoch in pbar:\n",
    "    loss_epoch = 0.\n",
    "    for batch in dm.train_dataloader():\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        atomic_contributions = painn(\n",
    "            atoms=batch.z,\n",
    "            atom_positions=batch.pos,\n",
    "            graph_indexes=batch.batch\n",
    "        )\n",
    "        preds = post_processing(\n",
    "            atoms=batch.z,\n",
    "            graph_indexes=batch.batch,\n",
    "            atomic_contributions=atomic_contributions,\n",
    "        )\n",
    "\n",
    "        loss_step = F.mse_loss(preds, batch.y, reduction='sum')\n",
    "        loss = loss_step / len(batch.y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch += loss_step.detach().item()\n",
    "\n",
    "\n",
    "    loss_epoch /= len(dm.data_train)\n",
    "    train_losses.append(loss_epoch)\n",
    "\n",
    "    swag_model.collect_model(painn)\n",
    "\n",
    "    pbar.set_postfix_str(f'Train loss: {loss_epoch:.3e}')\n",
    "\n",
    "# Evaluate SWAG model\n",
    "# Sample weights from SWAG posterior\n",
    "num_samples = 30  # Number of posterior samples\n",
    "swag_model.eval()\n",
    "mae = 0\n",
    "\n",
    "num_test_samples = len(dm.test_dataloader().dataset)\n",
    "all_preds = torch.zeros(num_samples, num_test_samples).to(device)  # To store predictions for each model sample\n",
    "all_true_labels = torch.zeros(num_test_samples).to(device)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    swag_model.sample()  # Sample weights\n",
    "    preds_for_sample = torch.zeros(num_test_samples).to(device)\n",
    "    idx = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dm.test_dataloader():\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            atomic_contributions = swag_model(\n",
    "                atoms=batch.z,\n",
    "                atom_positions=batch.pos,\n",
    "                graph_indexes=batch.batch,\n",
    "            )\n",
    "            preds = post_processing(\n",
    "                atoms=batch.z,\n",
    "                graph_indexes=batch.batch,\n",
    "                atomic_contributions=atomic_contributions,\n",
    "            )\n",
    "\n",
    "            batch_size = preds.size(0)\n",
    "            preds_for_sample[idx:idx + batch_size] = preds.squeeze()\n",
    "            all_true_labels[idx:idx + batch_size] = batch.y.squeeze()\n",
    "\n",
    "            idx += batch_size\n",
    "\n",
    "    all_preds[i] = preds_for_sample\n",
    "avg_preds = torch.mean(all_preds, dim=0)   \n",
    "mae = F.l1_loss(avg_preds, all_true_labels, reduction='sum')\n",
    "\n",
    "mae /= (len(dm.data_test))\n",
    "unit_conversion = dm.unit_conversion[args.target]\n",
    "\n",
    "test_mae = unit_conversion(mae)\n",
    "print(f'Test MAE (SWAG): {test_mae:.3f}')\n",
    "\n",
    "# Save to a text file\n",
    "# output_text = f'Test MAE: {test_mae:.3f}'\n",
    "# with open(f\"test_results_swag_{args.swag_lr}lr.txt\", 'w') as file:\n",
    "#     file.write(output_text)\n",
    "\n",
    "# plt.plot(train_losses, label=\"Train Loss\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.legend()\n",
    "# plt.title(\"Training and Validation Loss\")\n",
    "# plt.savefig(f'loss_plot_swag_{args.swag_lr}lr.png')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
